# Yaoyu Zhang 
#### zhyy.sjtu@sjtu.edu.cn 
#### 324 No.5 Science Buildings, Shanghai Jiao Tong University

## Academic experience
### • Tenure-track Associate Professor, 2020. 9 – 
#### &ensp; Institute of Natural Sciences&School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai

### • Member, 2019. 9 – 2020. 7
#### &ensp; School of Mathematics, Institute for Advanced Study, Princeton, New Jersey

### • Post-doctoral Associate, 2016-2019
#### &ensp; NYUAD Institute, New York University Abu Dhabi and Courant Institute of Mathematical Sciences, New York University
#### &ensp; Supervisors: David Cai, David W. McLaughlin #

## Education
### • Ph.D in Mathematics, 2012-2016
#### &ensp; School of Mathematical Sciences and Institute of Natural Sciences, Shanghai Jiao Tong University, Shanghai, China
#### &ensp; Advisors: Douglas Zhou, David Cai
#### &ensp; PhD thesis (in Chinese): Sampling Artifacts of Granger Causality and the Reliable Reconstruction of Neuronal Network Connectivity

### • B.S. in Applied Physics, minor in Applied Mathematics, 2008-2012
#### &ensp; Zhiyuan College, Shanghai Jiao Tong University, Shanghai, China

## Research Interests
### Deep Learning Theory
#### •	Condensation phenomenon
#### •	Loss landscape, generalization & training dynamics
### Computational Neuroscience
#### •	Robustness of cortical circuits


## Publications
### Highlights in deep learning theory
### **Optimistic Estimate (generalization)**
#### **Optimistic Estimate**: estimate of the smallest possible sample size for recovering a target for nonlinear regression.
#### **[Short paper]** **Yaoyu Zhang***, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, Optimistic Estimate Uncovers the Potential of Nonlinear Models. arXiv:2307.08921, (2023). [[web]](https://arxiv.org/abs/2307.08921) [[pdf]](https://arxiv.org/pdf/2307.08921)
#### *A conceptual leap from long paper. Establish optimistic estimate and estimate the optimistic sample size for deep models, matrix factorization model and DNNs.*
#### **[Long paper]** **Yaoyu Zhang***, Zhongwang Zhang, Leyang Zhang, Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, Linear Stability Hypothesis and Rank Stratification for Nonlinear Models. arXiv:2211.11623, (2022). [[web]](https://arxiv.org/abs/2211.11623) [[pdf]](https://arxiv.org/pdf/2211.11623)
#### *An early manuscript containing many detailed technical results, but lack the key concept of optimistic estimate.*
#### **[Local recovery guarantee at overparameterization]** Leyang Zhang, **Yaoyu Zhang**, Tao Luo, Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. arXiv:2309.00508, (2023). [[web]](https://arxiv.org/abs/2309.00508) [[pdf]](https://arxiv.org/pdf/2309.00508)

### **Embedding Principle (loss landscape)**
#### **Embedding Principle (in width)**: loss landscape of any DNN contains all critical points of all narrower DNNs.
#### **[Short paper]** **Yaoyu Zhang***, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu*, Embedding Principle of Loss Landscape of Deep Neural Networks. NeurIPS 2021 spotlight. [[web]](https://arxiv.org/abs/2105.14573) [[pdf]](https://arxiv.org/pdf/2105.14573)
#### *Prove the Embedding Principle by the multi-step compositional embedding and unravel its practical implications to optimization, training&generalization and pruning.*
#### **[Long paper]** **Yaoyu Zhang***, Yuqing Li, Zhongwang Zhang, Tao Luo, Zhi-Qin John Xu*, Embedding Principle: a hierarchical structure of loss landscape of deep neural networks. 	arXiv:2111.15527, (2021). [[web]](https://arxiv.org/abs/2111.15527) [[pdf]](https://arxiv.org/pdf/2111.15527)
#### *Extend the results in short paper, formally define the critical embedding and discover a wider class of general compatible critical embeddings.*
#### **[Embedding Principle in depth]** Zhiwei Bai, Tao Luo, Zhi-Qin John Xu*, **Yaoyu Zhang***, Embedding Principle in Depth for the Loss Landscape Analysis of Deep Neural Networks, 	arXiv:2205.13283, (2022). [[web]](https://arxiv.org/abs/2205.13283) [[pdf]](https://arxiv.org/pdf/2205.13283)
#### *Establish the Embedding Principle in depth.*

### **Phase diagram (dynamics)**
#### **Phase diagram**: a diagram showing the transition between linear (NTK/kernel/lazy) regime, critical (mean-field) regime or condensed regime depending on initialization hyperparameters for NNs at the infinite-width limit.
#### **[Two-layer]** Tao Luo#, Zhi-Qin John Xu#, Zheng Ma, **Yaoyu Zhang***,  Phase diagram for two-layer ReLU neural networks at infinite-width limit, Journal of Machine Learning Research (JMLR) 22(71):1−47, (2021) [[web]](https://arxiv.org/abs/2007.07497) [[pdf]](https://arxiv.org/pdf/2007.07497)
#### *A map for realizing different training and implicit regularization effect.*
#### **[Three-layer]** Hanxu Zhou, Zhou Qixuan, Zhenyuan Jin, Tao Luo, **Yaoyu Zhang**, Zhi-Qin Xu*, Empirical phase diagram for three-layer neural networks with infinite width, NeurIPS 2022 [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2022/file/a71c1931d3fb8ba564f7458d0657d0b1-Paper-Conference.pdf).

### **Frequency Principle (dynamics&implicit bias)**
#### **Frequency Principle**: DNNs often learn target function from low to high frequencies. 
#### **[First paper]** Zhiqin Xu, **Yaoyu Zhang**, Yanyang Xiao, Training Behavior of Deep Neural Network in Frequency Domain, International Conference on Neural Information Processing (ICONIP), pp. 264-274, 2019. (arXiv:1807.01251, Jul 2018) [[web]](https://arxiv.org/abs/1807.01251) [[pdf]](https://arxiv.org/pdf/1807.01251)
#### *Empirically discovering the Frequency Principle in simple datasets (specifically 1-d sythetic data).*
#### **[2021 World Artificial Intelligence Conference Youth Outstanding Paper Nomination Award]** Zhi-Qin John Xu*, **Yaoyu Zhang**, Tao Luo, Yanyang Xiao, Zheng Ma, Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks, Communications in Computational Physics (CiCP) 28(5). 1746-1767, 2020. [[web]](https://arxiv.org/abs/1901.06523) [[pdf]](https://arxiv.org/pdf/1901.06523)
#### *Extensively demonstrate the Frequency Principle in high-dimensional real datasets with an intuitive theoretical explanation.*
#### **[Linear Frequency Principle]** **Yaoyu Zhang**, Tao Luo, Zheng Ma, Zhi-Qin John Xu*, Linear Frequency Principle Model to Understand the Absence of Overfitting in Neural Networks, Chinese Physics Letters (CPL) 38(3), 038701, 2021. [[web]](https://arxiv.org/abs/2102.00200) [[pdf]](https://arxiv.org/pdf/2102.00200)
#### *Propose a linear frequency principle (LFP) model to quantitatively understand the training and generalization consequence of the Frequency Principle.*

### Deep learning theory
#### •	**[Initial condensation]** Zhi-Qin John Xu*#, Hanxu Zhou#, Tao Luo, **Yaoyu Zhang***, Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. NeurIPS 2022 [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2022/file/0f4d1fc085b7504c140e66bb26ed8842-Paper-Conference.pdf) ;arXiv:2105.11686, (2021) [[web]](https://arxiv.org/abs/2105.11686) [[pdf]](https://arxiv.org/pdf/2105.11686)
#### •	**[DNN vs. Ritz-Galerkin]** (Alphabetic order) Jihong Wang, Zhi-Qin John Xu*, Jiwei Zhang*, **Yaoyu Zhang**, Implicit bias with Ritz-Galerkin method in understanding deep learning for solving PDEs, arXiv:2002.07989, 2020. (accepted by CSIAM Trans. Appl. Math. 2021) [[web]](https://arxiv.org/abs/2002.07989) [[pdf]](https://arxiv.org/pdf/2002.07989)
#### •	**[F-Principle theory for general DNN]** (Alphabetic order) Tao Luo, Zheng Ma, Zhi-Qin John Xu, **Yaoyu Zhang**, Theory of the Frequency Principle for General Deep Neural Networks, CSIAM Trans. Appl. Math. 2 (2021), pp. 484-507 [[web]](https://arxiv.org/abs/1906.09235) [[pdf]](https://arxiv.org/pdf/1906.09235)
#### •	**[Initialization effect]** **Yaoyu Zhang**, Zhi-Qin John Xu*, Tao Luo, Zheng Ma, A type of generalization error induced by initialization in deep neural networks, Mathematical and Scientific Machine Learning (MSML), 2020. [[web]](https://arxiv.org/abs/1905.07777) [[pdf]](https://arxiv.org/pdf/1905.07777)
#### •	**[Derivation of LFP model]** (Alphabetic order) Tao Luo, Zheng Ma, Zhi-Qin John Xu, **Yaoyu Zhang**, On the exact computation of linear frequency principle dynamics and its generalization, arXiv:2010.08153, 2020. [[web]](https://arxiv.org/abs/2010.08153) [[pdf]](https://arxiv.org/pdf/2010.08153)
#### •	**[Limit convergence rate decay for Frequency Principle]** (Alphabetic order) Tao Luo, Zheng Ma, Zhiwei Wang, Zhi-Qin John Xu, and Yaoyu Zhang. Fourier-domain Variational Formulation and Its Well-posedness for Supervised Learning. arXiv:2012.03238, 2020. [[web]](https://arxiv.org/abs/2012.03238) [[pdf]](https://arxiv.org/pdf/2012.03238). See also: An Upper Limit of Decaying Rate with Respect to Frequency in Deep Neural Network. arXiv:2105.11675, 2020. [[web]](https://arxiv.org/abs/2105.11675) [[pdf]](https://arxiv.org/pdf/2105.11675)

### Deep learning in application
#### •	**[DiDo for industrial design]** Lulu Zhang, Zhi-Qin John Xu*, **Yaoyu Zhang***,Data-informed Deep Optimization, arXiv:2107.08166, (2021). [[web]](https://arxiv.org/abs/2107.08166) [[pdf]](https://arxiv.org/pdf/2107.08166)
#### •	**[MOD-Net for PDEs]** Lulu Zhang, Tao Luo, **Yaoyu Zhang**, Zhi-Qin John Xu*, Zheng Ma*, MOD-Net: A Machine Learning Approach via Model-Operator-Data Network for Solving PDEs, arXiv:2107.03673, (2021). [[web]](https://arxiv.org/abs/2107.03673) [[pdf]](https://arxiv.org/pdf/2107.03673)
#### •	**[DeepCombustion0.0 for combustion simulation]** Tianhan Zhang, **Yaoyu Zhang***, Weinan E, Yiguang Ju, A deep learning-based ODE solver for chemical kinetics, arXiv:2012.12654, (2020). [[web]](https://arxiv.org/abs/2012.12654) [[pdf]](https://arxiv.org/pdf/2012.12654)（Zhang, Tianhan, Yaoyu Zhang, E. Weinan, and Yiguang Ju. "A deep learning-based ode solver for chemical kinetics." In AIAA Science and Technology Forum and Exposition, AIAA SciTech Forum 2021. American Institute of Aeronautics and Astronautics Inc, AIAA, 2021.）
#### • **[DeePMR for chemical kinetics reduction]** Zhiwei Wang, **Yaoyu Zhang**, Yiguang Ju, Weinan E, Zhi-Qin John Xu*, Tianhan Zhang*, A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics, arXiv:2201.02025, (2021). [[web]](https://arxiv.org/abs/2201.02025) [[pdf]](https://arxiv.org/pdf/2201.02025)

### Computational neuroscience
#### •	**Yaoyu Zhang**, Lai-Sang Young, DNN-Assisted Statistical Analysis of a Model of Local Cortical Circuits, Scientific Reports 10, 20139, 2020.
#### •	**Yaoyu Zhang**, Yanyang Xiao, Douglas Zhou, David Cai, Spike-Triggered Regression for Synaptic Connectivity Reconstruction in Neuronal Networks, Frontiers in Computational Neuroscience 11, 101, 2017.
#### •	**Yaoyu Zhang**, Yanyang Xiao, Douglas Zhou, David Cai, Granger Causality Analysis with Nonuniform Sampling and Its Application to Pulse-coupled Nonlinear Dynamics, Physical Review E 93, 042217, 2016.
#### •	Douglas Zhou, **Yaoyu Zhang**, Yanyang Xiao, David Cai, Analysis of Sampling Artifacts on the Granger Causality Analysis for Topology Extraction of Neuronal Dynamics, Frontiers in Computational Neuroscience 8, 75, 2014.
#### •	Douglas Zhou, **Yaoyu Zhang**, Yanyang Xiao, David Cai, Reliability of the Granger Causality Inference, New Journal of Physics 16 (4), 043016, 2014. 
#### •	Douglas Zhou, Yanyang Xiao, **Yaoyu Zhang**, Zhiqin Xu, David Cai, Granger Causality Network Reconstruction of Conductance-Based Integrate-and-Fire Neuronal Systems, PloS One 9 (2), e87636, 2014.
#### •	Douglas Zhou, Yanyang Xiao, **Yaoyu Zhang**, Zhiqin Xu, David Cai, Causal and Structural Connectivity of Pulse-coupled Nonlinear Networks, Physical Review Letters 111 (5), 054102, 2013.

### Online talks
#### •	[Dynamics of Deep Neural Networks--A Fourier Analysis Perspective.](https://www.ias.edu/video/postdoc/2019/1004-YaoyuZhang) Short talks by postdoctoral members at IAS, 2019.
#### •	[A Type of Generalization Error Induced by Initialization in Deep Neural Networks.](https://www.youtube.com/watch?v=t40JlC8xtkY) MSML2020 Paper Presentation.
#### •	[Embedding Principle of Loss Landscape of Deep Neural Networks.](https://www.bilibili.com/video/BV1x44y1x7Qg) 机器学习联合研讨计划 2021.
#### •	[深度学习损失景观的嵌入原则.](https://app6ca5octe2206.pc.xiaoe-tech.com/detail/v_61b9663fe4b0ad2813b01a71/3?fromH5=true) 2021 NeurIPS MeetUp China.

